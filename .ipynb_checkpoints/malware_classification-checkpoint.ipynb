{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import tensorflow_datasets as tfds\n",
    "from sklearn import preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import operator\n",
    "import itertools\n",
    "import csv\n",
    "import pandas as pd\n",
    "\n",
    "freq_Per_Family={}\n",
    "def Reset():\n",
    "    freq_Per_Family={}\n",
    "\n",
    "def CountFrequency(my_list): \n",
    "    # Creating an empty dictionary  \n",
    "    freq = {} \n",
    "    # Updating the value of the count if the operation exists \n",
    "    for item in my_list: \n",
    "        if (item in freq): \n",
    "            freq[item] += 1\n",
    "    # Adding the operation to the dictionary if it doesn't exist \n",
    "        else: \n",
    "            freq[item] = 1\n",
    "\n",
    "    for item in my_list: \n",
    "        if (item in freq_Per_Family): \n",
    "            freq_Per_Family[item] += 1\n",
    "    # Adding the operation to the dictionary if it doesn't exist \n",
    "        else: \n",
    "            freq_Per_Family[item] = 1\n",
    "    return freq\n",
    "\n",
    "#Fucntion to return a list path of all the files in a folder \n",
    "def ListOffiles(path):\n",
    "    dirListing = os.listdir(path)\n",
    "    return dirListing\n",
    "\n",
    "def GetFrequencies(path):\n",
    "    list_Of_Dict_Each_Family =[]\n",
    "    listOfFiles = ListOffiles(path)\n",
    "    # Iterating over all files \n",
    "    for items in listOfFiles:\n",
    "        if items.endswith('.txt'):\n",
    "            filepath = path + '/' + items\n",
    "            my_list = [] \n",
    "            #Open the cuurent file\n",
    "            with open(filepath) as fp:\n",
    "                line = fp.readline()\n",
    "                cnt = 1\n",
    "                while line:\n",
    "                    line = fp.readline()\n",
    "                    #Store the operation in the line to a list\n",
    "                    my_list.append(line.rstrip('\\n'))\n",
    "                    cnt += 1\n",
    "            #Delete last element if it is empty i.e empty last line\n",
    "            if(my_list[-1]) == \"\":\n",
    "                del my_list[-1]\n",
    "            total_lines = cnt  \n",
    "            #Count frequency of each operation in the file\n",
    "            freq_Per_File={}\n",
    "            freq_Per_File=CountFrequency(my_list)\n",
    "            #Find percentage from the counted frequencies \n",
    "            for key, value in freq_Per_File.items(): \n",
    "                freq_Per_File[key] = (value/total_lines)\n",
    "            list_Of_Dict_Each_Family.append(freq_Per_File) \n",
    "\n",
    "    list_inputs= Calculate_Input()\n",
    "    l_list = UpdateFrequencies(list_Of_Dict_Each_Family,list_inputs)\n",
    "    return l_list\n",
    "\n",
    "def UpdateFrequencies(list_Of_Dict_Each_Family,list_inputs):\n",
    "    temp_list=[]\n",
    "    size = len(list_Of_Dict_Each_Family)\n",
    "    for k in range (0,size,1):\n",
    "        temp_dict={}\n",
    "        Misc_sum = 0\n",
    "        for key,value in list_Of_Dict_Each_Family[k].items(): \n",
    "            if (key in list_inputs): \n",
    "                temp_dict.update({key:value})\n",
    "            else:\n",
    "                Misc_sum+=value   \n",
    "        temp_dict.update({\"Misc\":Misc_sum})\n",
    "\n",
    "        #Adding 0 to files which don't have all the 'input list' elements\n",
    "        for items in list_inputs:\n",
    "            if items not in temp_dict.keys():\n",
    "                temp_dict.update({items:0})\n",
    "        d={}\n",
    "        for key in sorted(temp_dict.keys()):\n",
    "            d.update({key:temp_dict[key]})\n",
    "        temp_list.append(d)\n",
    "    return temp_list,list_inputs\n",
    "\n",
    "def Calculate_Input():\n",
    "    input_dict=[]\n",
    "    list_inputs=[]\n",
    "    #Sort in decending order\n",
    "    dict(sorted(freq_Per_Family.items(), key=operator.itemgetter(1),reverse=True))\n",
    "    temp_sum=0\n",
    "    #Temp dictionary to calulate no. of occurences of Misc operations \n",
    "    temp_dict = {k: freq_Per_Family[k] for k in list(freq_Per_Family)[29:]}\n",
    "    for key,value in temp_dict.items(): \n",
    "        temp_sum+=int(value)\n",
    "    # Temp input dictionary with no. of occurences of 29 most common operations\n",
    "    input_dict = {k: freq_Per_Family[k] for k in list(freq_Per_Family)[:29]}\n",
    "    # Input dictionaries with no. of occurences of most 29 common operations + Misc (All remaining operations)\n",
    "    input_dict.update({\"Misc\" : temp_sum})\n",
    "    #Storing the inputs in a list \n",
    "    for key,values in input_dict.items(): \n",
    "        list_inputs.append(key)\n",
    "    return list_inputs\n",
    "\n",
    "# Driver function \n",
    "if __name__ == \"__main__\": \n",
    "    #Path of the 1st directory containing all the malware samples\n",
    "    path1 = \"/Users/aryan/Desktop/MalwareNeuralNetwork-master/zbot\"\n",
    "    list_Of_Dict_Family_1,list_inputs= GetFrequencies(path1)\n",
    "    Reset()\n",
    "    path2 = \"/Users/aryan/Desktop/MalwareNeuralNetwork-master/zeroaccess\"\n",
    "    list_Of_Dict_Family_2,list_inputs= GetFrequencies(path2)\n",
    "\n",
    "    sorted_list=sorted(list_inputs)\n",
    "    sorted_list.append(\"Target\")\n",
    "\n",
    "    with open('malware.csv', 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "#         writer.writerow(sorted_list)\n",
    "        lst=[]\n",
    "        size=len(list_Of_Dict_Family_1)\n",
    "        for k in range (0,size,1):\n",
    "            for key,values in list_Of_Dict_Family_1[k].items(): \n",
    "                lst.append(values)\n",
    "            lst.append(\"0\")\n",
    "            writer.writerow(lst)\n",
    "            lst=[]\n",
    "        size=len(list_Of_Dict_Family_2)\n",
    "        lst=[]\n",
    "        #Family 2\n",
    "        for k in range (0,size,1):\n",
    "            for key,values in list_Of_Dict_Family_2[k].items(): \n",
    "                lst.append(values)\n",
    "            lst.append(\"1\")\n",
    "            writer.writerow(lst)\n",
    "            lst=[]\n",
    "\n",
    "        # df = pd.read_csv('malware.csv',header=None)\n",
    "        # print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# preprocessing the data\n",
    "raw_cvs_data = np.loadtxt('malware.csv',delimiter=',')\n",
    "#All except last column \n",
    "unscaled_inputs_all = raw_cvs_data[:-1]\n",
    "#Last column is the target\n",
    "targets_all = raw_cvs_data[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "arrays used as indices must be of integer (or boolean) type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-b531c6d0821a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m \u001b[0mshuffled_indices\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mscaled_inputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffled_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 25\u001b[0;31m \u001b[0mshuffled_targets\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtargets_equal_priors\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mshuffled_indices\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     26\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m \u001b[0;31m#splitting the data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: arrays used as indices must be of integer (or boolean) type"
     ]
    }
   ],
   "source": [
    "# balancing the dataset\n",
    "num_one_targets = int(np.sum(targets_all)) # count how many targets are 1\n",
    "zero_targets_counter = 0 # counter for target 0\n",
    "\n",
    "indices_to_remove = [] # remove extra input/target pairs for balance \n",
    "\n",
    "# count the number of targets 0, when get same amount of target 1 and 0, make entries where target is zero\n",
    "for i in range(targets_all.shape[0]):\n",
    "        if targets_all[i] == 0:\n",
    "            zero_targets_counter +=1\n",
    "            if zero_targets_counter > num_one_targets:\n",
    "                indices_to_remove.append(i)\n",
    "\n",
    "unscaled_inputs_equal_priors = np.delete(unscaled_inputs_all,indices_to_remove, axis = 0)\n",
    "targets_equal_priors = np.delete(targets_all, indices_to_remove, axis = 0)\n",
    "\n",
    "#Standardize the input using sklearn\n",
    "scaled_inputs = preprocessing.scale(unscaled_inputs_equal_priors)\n",
    "\n",
    "#Shuffle the data \n",
    "shuffled_indices = np.arange(scaled_inputs.shape[0])\n",
    "np.random.shuffle(shuffled_indices)\n",
    "\n",
    "shuffled_indices = scaled_inputs[shuffled_indices]\n",
    "shuffled_targets = targets_equal_priors[shuffled_indices]\n",
    "\n",
    "#splitting the data \n",
    "samples_count = shuffled_inputs_shape[0]\n",
    "train_samples_count = int(0.8*samples_count)\n",
    "validtion_samples_count = int(0.1*samples_count)\n",
    "\n",
    "test_samples_count = samples_count - train_samples_count - validation_samples_count\n",
    "\n",
    "train_inputs = shuffled_inputs[:train_samples_count]\n",
    "train_targets = shuffled_targets[:train_samples_count]\n",
    "\n",
    "validation_inputs =  shuffled_inputs[train_samples_count:train_samples_count+validation_samples_count]\n",
    "validation_targets =  shuffled_targets[train_samples_count:train_samples_count+validation_samples_count]\n",
    "\n",
    "test_inputs=shuffled_inputs[train_samples_count+validation_samples_count:]\n",
    "test_targets=shuffled_targets[train_samples_count+validation_samples_count:]\n",
    "\n",
    "print(np.sum(train_targets),train_samples_count,np.sum(train_targets) / train_samples_count)\n",
    "print(np.sum(validation_targets),train_samples_count,np.sum(validation_targets) / validtion_samples_count)\n",
    "print(np.sum(test_targets),test_samples_count,np.sum(test_targets) / test_samples_count)\n",
    "\n",
    "\n",
    "np.savez('malware_data_train', inputs=train_inputs, targets=train_targets)\n",
    "np.savez('malware_data_validation', inputs=validation_inputs, targets=validation_targets)\n",
    "np.savez('malware_data_test', inputs=test_inputs, targets=test_targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# seperate into validation / traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# validation"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python3-TensorFlow2",
   "language": "python",
   "name": "python3-tensorflow2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
